---
title: "Cross-Validation for PCA"
author: "Noah Scheider - Emanuele Sorgente - Eliott Van Dieren"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(mvtnorm)
```

**Work for 2.12-4.12:**

<span style="color:brown">Emanuele</span>
<span style="color:orange">Noah</span>
<span style="color:gray">Eliott</span>


* Run simulation study on synthesized data (- Emphasize on noise, correlation, hyperparameter r, comparing with elbow plots, use the same generated data for the different methods) - document along the way <span style="color:orange">Noah</span>, <span style="color:gray">Eliott</span>

* Report:
  
  * For **each** method explanation, pseudo-code, simulation & documentation, conclusion <span style="color:orange">Noah</span>, <span style="color:gray">Eliott</span>, <span style="color:brown">Emanuele</span>
  * Extend [1.3 Simulation dataset description] <span style="color:gray">Eliott</span>
  * Overall conclusion

  
* Polishing:

  * Consider and incorporate "General Feedback for Small Projects" - Manual 11 (Copied to our github)

* Questions:
  
  * MatrixCompletion: Tolerance and CV
  * MatrixCompletion: To what extent does noise influnence -_-
 


## 1. Introduction

## 1.1 Problem description

When dealing with modern large datasets, the observations' dimensionality often becomes a problem regarding the interpretability of the dataset, or for its vizualisation, especially during the data exploration phase. The Principal Component Analysis (PCA) is a statistical technique used to reduce the dimension of the dataset, while trying to keep as much information as possible from the original dataset (aka variance). An example of PCA usage is bringing a high-dimension dataset to $\mathbb{R}^2$ or $\mathbb{R}^3$ to vizualise its datapoints and afterwards identifying clusters. It does so by computing Principal components from the dataset and then projecting datapoints to this new basis.

Mathematically speaking, we can define principal components as an orthonormal basis of unit vectors which sequentially capture the most variance in the data. Practically, the PCs can be found with the following optimization problems for a given centered Gaussian dataset $\mathbf x_1,\ldots,\mathbf x_N \in \mathbb{R}^p$: 
\[
\begin{split}
v_1 &= \underset{v, \| v \|=1}{\mathrm{arg\,max}} \mbox{ } v^\top \widehat{\boldsymbol{\Sigma}} v \\
v_2 &= \underset{v, \| v \|=1, v^\top v_1 = 0}{\mathrm{arg\,max}} v^\top \widehat{\boldsymbol{\Sigma}} v \\
&\;\;\vdots
\end{split}
\]

with $\widehat{\Sigma} = N^{-1}\sum_{n=1}^N \mathbf x_n \mathbf x_n^\top \in \mathbb{R}^{p\times p}$ being the empirical covariance matrix of the dataset. More information about Principal component analysis can be found in [Jolliffe, 2002](https://link.springer.com/book/10.1007/b98835) or directly on [Wikipedia](https://en.wikipedia.org/wiki/Principal_component_analysis).


```{r, echo=FALSE, fig.align = 'center', out.width = "25%", fig.cap = "Example of 2 PCs on a given dataset in $R^2$"}
knitr::include_graphics("./Figures/PCA_p_2_example.png")
```


This project will focus on finding the optimal number of principal components ($r$), which dictates a trade-off between the model's complexity and its interpretability. In fact, higher the $r$, higher the explained variance, but less interpretable the dataset. To optimally choose the number of PCs, several techniques have been created, such as the method of percentage of variance explained, the scree-plot method or Cross-validation for PCA.

In this report, one will first find a presentation of each Cross-Validation method for PCA with an initial description, followed by a pseudo-code in [2. Cross-validation on PCA methods]. Then, those methods will be ran on simulated datasets with different parameters in [3. Simulation and comparison of CV methods]. Furthermore, the percentage of variance explained and the scree-plots methods will be described and tested in [4. Other PCs number selection methods]. Finally, the conclusion will wrap up the take-home messages as well as next steps which could be conducted regarding this subject.

## 2. Cross-validation on PCA methods

### 2.1 Naïve Approach

A widely used approach for Cross-validation in PCA is a method that has the right intentions but fails at a critical point :it does not take into account any variance-bias tradeoff, meaning that the optimal solution will always be $r = R$. We start with the usual conventions by denoting $X \in \mathbb{R}^{n \times p}$ our data with $n$ observations and $p$ variables, then as presented in [Notes 06 - T. Masak](https://htmlpreview.github.io/?https://github.com/TMasak/StatComp/blob/master/Notes/06_CV.html#no.-of-component-selection-for-pca), below is the pseudo-code of the Naïve approach:

  * split data $\mathbf{X}$ into $K$ folds $J_1,\ldots,J_K$ (row indices)
  * **for** $k=1,\ldots,K$:
    - solve $\widehat{\mathbf{L}} = \underset{\mathrm{rank}(\mathbf L) = r}{\mathrm{arg\,min}} \|\mathbf X[J_k^c,:] -\mathbf L \|_2^2$
    - calculate $Err_k(r) = \frac{1}{|J_k|}\sum_{m \in J_k} \| x^m - P_{\widehat{\mathbf{L}}} x^m \|_2^2$
  * **end for**
  * choose $\widehat{r} = \underset{r}{\mathrm{arg\,min}} \sum_{k=1}^K Err_k(r)$

A numerical method to solve the least square problem in the top of the algorithm would be to compute the SVD decomposition of $\mathbf X[J_k^c,:]$ and truncate the rank to $r$ by setting the $p-r$ smallest singular values to zero. To estimate $\mathbf{x^{m}}, \forall m \in J_k$ with a lower dimensional version, we project it with the matrix $P_{\mathbf{\widehat{L}}}$ which can be computed via the the R built-in function `prcomp()` the returns among other objects a basis for the dimension of $\widehat{\mathbf L}$. The promised issue occurs when computing the error measure without further to-do. Thus the error obviously shrinks when $\widehat{\mathbf L}$ is allowed to have higher dimensions and converges to $0$ when its number of dimensions equals those of the initial data set $\mathbf{X}$, as the SVD will be a nearly exact representation of the matrix $\mathbf{X}$.

### 2.2 Artificially turn the unsupervised problem into a supervised one

This method tries to "correct" the [2.1 Naïve Approach] by inducing a Bias-Variance trade-off for the $Err_k$ function. Let again $\mathbf{X} \in \mathbb{R}^{n \times p}$ be our Multivariate Gaussian dataset with $n$ observations and $p$ variables, mean $\mu$ and covariance $\Sigma$. As described in [Notes 06 - T. Masak](https://htmlpreview.github.io/?https://github.com/TMasak/StatComp/blob/master/Notes/06_CV.html#no.-of-component-selection-for-pca), we implement this method as below :

  * split data into $K$ folds $J_1,\ldots,J_K$ (row indices)
  * **for** $k=1,\ldots,K$:
    - Compute $\hat{\mu}$ and $\hat{\Sigma} := \hat{\Sigma}_r$ (the empirical estimator truncated to rank r) on $\mathbf{X}\setminus J_k$. + ADD NOISE
    - Split data points $\mathbf{x}^m\in \mathbb{R}^{p}, m \in J_k$ into a "missing" part $\mathbf{x}_{miss}$ that will be used for validation and an "observed" part $\mathbf{x}_{obs}$ in order to estimate its counterpart$
    - Predict the missing part from the observed part using the $\hat{\mu}$ and $\hat{\Sigma}_r$
    - That is compute $\hat{\mathbf{x}}^m_{miss} = \mathbb{E}_{\hat{\mu},\hat{\Sigma}_r}[\mathbf{X}^m_{miss}|\mathbf{X}^m_{obs}=\mathbf{x}^m_{obs}] = \hat{\mu}_{miss} + \hat{\Sigma}_{miss,obs}\hat{\Sigma}^\dagger_{obs}(\mathbf{x}^m_{obs}-\hat{\mu}_{obs})$, where $\hat{\Sigma}^\dagger_{obs}$ is considered the Pseudoinverse of $\hat{\Sigma}_{obs,obs}$ and $\hat{\mu}_{miss},\hat{\mu}_{obs}$ is $\hat{\mu}$ split according to the "missed" and "observed" variable indices.
  * **end for**
  * choose $\widehat{r} = \underset{r}{\mathrm{arg\,min}} \sum_{k=1}^K Err_k(r)$, where for instance $Err_k(r)$ can be considered as the MSE for every fold $J_k$, e.g. $Err_k(r)=\frac{1}{|J_k|}\sum_{m\in J_k} \|\hat{\mathbf{x}}^m_{miss}-\mathbf{x}^m_{miss}\|^2_2$

Before continuing with our simulations we want to emphasize on some more and some less obvious clues in order to avoid any confusion. When we truncate $\hat{\Sigma}$ to $\hat{\Sigma}_r$ in every fold, we actually take its SVD decomposition and "cut off" its $p-r$ smallest singular values. Hence, we reduce the rank of the covariance of $\mathbf{X}$, but we preserve its dimension which allows us to estimate $\mathbf{x}^m_{miss}$ in the later part of the algorithm. When splitting the observations in every iteration of the for loop, it naturally comes to mind if we should always take the same or a different (random) split. We went for the first possibility as that additional random factor would be negligible by the SLLN. Nevertheless, it makes sense to keep two equally sized parts in order to have a proper estimation basis without overestimating.
When we estimate $\hat{\mathbf{x}}^m_{miss}$ we use a specific formula that emerges from the law of our data set $\mathbf{X}$. Hence, it is only applicable in this form to Gaussian distributed observations $\mathbf{x} \in \mathbb{R}^p$. The estimation also makes use of the Pseudoinverse of $\hat{\Sigma}_{obs,obs}$ denoted by $\hat{\Sigma}^+_{obs,obs}$. We use this convention in order to avoid singularity conflicts. The Pseudoinverse is easily callable in R by the function `ginv()` embedded in the `MASS` package.

### 2.3 Missing data approach

Here it is assumed that the rows $X_{n} \in \mathbb{R}^p, n=1,…,N$ of the data matrix $X=(X_{n,j})^{N,p}_{n,j=1}$ are i.i.d. realizations of a Gaussian random variable $X$. That is, the rows have with a mean $\mu \in \mathbb{R}^p$ and a covariance $\Sigma \in \mathbb{R}^{p \times p}$. We will assume that $rank(\Sigma)=r$ for $r=1,…,R$, and compare how well different values of $r$ fit the data. As described in [Notes 06 - T. Masak](https://htmlpreview.github.io/?https://github.com/TMasak/StatComp/blob/master/Notes/06_CV.html#no.-of-component-selection-for-pca), this approach is based on computing the estimators $\hat{\Sigma}$ and $\hat{\mu}$ on observed data by the EM algorithm and the set of missed and observed data is randomly selected for each fold $K_i$. Let us now describe the EM algorithm used in our case and then describe the Missing data approach algorithm.

Let $\tilde{X}$ be the dataset containing missing values randomly placed on the original $N \times p$ dataset. The EM algorithm works as follows :

  * Initialize $\hat{\mu} = \frac{1}{N}\sum_{n=1}^N(\tilde{X}^{obs}_{n})$ and $\hat{\Sigma} = I_{p\times p}$, where $\tilde{X}_{obs}$ is the dataset restricted to observed values.
  * Set $\hat{\Sigma}^{old} = C\hat{\Sigma}$, and $tol = 0.01$ for a given constant $C$ big enough to enter the while loop
  * **while** $(||\hat{\Sigma}-\hat{\Sigma}^{old}||_F > tol*||\hat{\Sigma}^{old}||_F)$ where $||.||_F$ is the Frobenius norm
    - Set $\hat{\Sigma}^{old} = \hat{\Sigma}$
    - Estimate $\tilde{X}_{miss}^{n}$ by a linear estimator : $\tilde{X}_{miss}^n = \hat{\mu}_{miss} + \hat{\Sigma}_{miss,obs} \hat{\Sigma}_{obs}^{\dagger}(\tilde{X}_{obs}^n-\hat{\mu}_{obs})$
    - Define $\hat{X}$ being $\tilde{X}$ with missing values filled by the linear estimator above.
    - Set $\hat{\mu} = mean(\hat{X})$, $\hat{\Sigma} = \frac{1}{N-1}\sum_{n=1}^N (\hat{X}_i-\hat{\mu})(\hat{X}_i-\hat{\mu})^T$, also known as the two MLE estimators
  * **end while**
  * return the estimated $\hat{\mu}, \hat{\Sigma}$

This method slightly differs from the one presented in [Notes 05 - T. Masak](https://htmlpreview.github.io/?https://github.com/TMasak/StatComp/blob/master/Notes/05_EM.html), mainly due to poor results when estimating $\hat{\Sigma}$. Another matrix approach for the EM algorithm was also tried from Machine Learning, Murphy but was also unsuccessfull when predicting $\hat{\Sigma}$.

Below is the pseudo-code for the Missing Data method :

  * split the dataset in $K$ folds by randomly selecting indices from the bivariate index set $J := \{(n,j) | n = 1,...,N, \quad, j=1,...,p\}$.
  * **for** $k=1,\ldots,K$:
    - Compute the bivariate set of missed data $\Omega$
    - Compute $\hat{\mu}$ and $\hat{\Sigma}$ by the EM algorithm with the dataset $X$ and $\Omega$ as inputs
    - Truncate $\hat{\Sigma}$ to a rank $r$ matrix $\hat{\Sigma}_r$ by eigenvalue decomposition, + ADD NOISE
    - Reuse the missed data $\Omega$ to split the data points $\mathbf{x}^m\in \mathbb{R}^{p}, m \in J_k$ into a "missing" part $\mathbf{x}_{miss}$ that will be used for validation and an "observed" part $\mathbf{x}_{obs}$ in order to estimate its counterpart
    - Predict the missing part from the observed part using the $\hat{\mu}$ and $\hat{\Sigma}_r$
    - That is compute $\hat{\mathbf{x}}^m_{miss} = \mathbb{E}_{\hat{\mu},\hat{\Sigma}_r}[\mathbf{X}^m_{miss}|\mathbf{X}^m_{obs}=\mathbf{x}^m_{obs}] = \hat{\mu}_{miss} + \hat{\Sigma}_{miss,obs}\hat{\Sigma}^\dagger_{obs}(\mathbf{x}^m_{obs}-\hat{\mu}_{obs})$, where $\hat{\Sigma}^\dagger_{obs}$ is considered the Pseudoinverse of $\hat{\Sigma}_{obs,obs}$ and $\hat{\mu}_{miss},\hat{\mu}_{obs}$ is $\hat{\mu}$ split according to the "missed" and "observed" part
  * **end for**   
  * choose $\widehat{r} = \underset{r}{\mathrm{arg\,min}} \sum_{k=1}^K Err_k(r)$, where for instance $Err_k(r)$ can be considered as the MSE for every fold $J_k$, e.g. $Err_k(r)=\frac{1}{|J_k|}\sum_{m\in J_k}


### 2.4 KDE modified approach
This approach is a modfication of the KDE  method. As indicated in [Notes 06 - T. Masak](https://htmlpreview.github.io/?https://github.com/TMasak/StatComp/blob/master/Notes/06_CV.html#no.-of-component-selection-for-pca) the goal is to minimize the rank $R$ in

\[
\mathbb{E} \| \widehat{C}_R - C \|_2^2 = \mathbb{E}\| \widehat{C}_R \| - 2 \mathbb{E} \langle \widehat{C}_R, C \rangle + \| C \|_2^2
\]

with $\widehat{C}_R$ being the truncated estimator having rank $R$.
Assuming that the mean is zero we, in our modification we'll get:
\[
\mathbb{E} \langle \widehat{C}_R^{(-n)}, X_n X_n^\top \rangle = \langle \underbrace{\mathbb{E}[ \widehat{C}_R^{(-n)}}_{\approx \mathbb{E} \widehat{C}_R}], \underbrace{\mathbb{E} [X_n X_n^\top}_{ = C}] \rangle \approx \mathbb{E} \langle \widehat{C}_R , X_n X_n^\top \rangle
\]

Therefore, since the truncated covariance estimator isn't linear we obtain an approximation. Based on this strategy we'll choose the rank $R$ to minimize our CV. Computing this approximation and averaging it for every observation, aswell as neglecting the $\| C \|_2^2$ (constant error) will produce a useful measure error. Overall, we implement this method as follows for every rank $R$:

* Estimate Covariance $\widehat{C}$ from given data $X$
* Decompose $\widehat{C}$ and set $p-R$ smallest eigenvalues to $0$
* Compute Frobenius-norm $\|\cdot\|=\|\cdot\|_F$
* **for** $j$ in $1,\dots,n$:
  - calculate $\widehat{C}_R^{(-j)}$ by estimating the covariance of $X^{(-j)}$, $j$ being the missing observation in $X$
  - reduce $\|\widehat{C}_R\|_F$ by $\frac{2}{n}\widehat{C}_R\cdot\widehat{C}_R^{(-j)}$
* **end**
* Choose $R$ according to the minimal error

Stepping into the for-loop we realize that it actually resembles a Leave-One-Out Cross-validation. In this Method we are trading off high computational cost against a stable solution.

### 2.5 Matrix completion method

This final methods is somehow a bit more general, as we do not need to assume any probability distribution of the data $X \in \mathbb{R}^{N\times p}$, but just that $X$ has a finite rank. Indeed, for a given data matrix $X$, we assume that a portion of this matrix is missed and that the remaining elements are observed. Let $\Omega$ be a bivariate index set for the observed data in $X$. The matrix completion method consists of finding a matrix $\mathbf M$ of rank $R$ such that :

\[
    \mathrm{arg \, min}_{\mathbf{M}=(M_{ij})_{i,j=1}^{N \times p}} \sum_{(i,j) \in \Omega} (X_{ij} - M_{ij})^2 \quad \mathrm{s.t.} \quad \mathrm{rank}(\mathbf M)=R
    \]

This optimization method gives us the best matrix $\mathbf M$ with rank $R$ with respect to least-square error on the observed data. However, finding the solution of such a optimization problem is $NP$-hard, so one can use the following iterative algorithm to find local best candidates for the matrix $\mathbb M$ :


  * Select an initial candidate $\mathbf{M}^{(0)}$ (here $\mathbf{M}^{(0)} = X$)
  * Set $l=1$
  * **do**
    - Compute the SVD of $\mathbf{M}^{(l-1)}$, i.e. $\mathbf{M}^{(l-1)} = UDV^T$
    - Copy $D$ into $\widetilde{D}$ and set the $p-R$ diagonal elements of $\widetilde{D}$ to zero (truncating to rank $R$)
    - Set $\widetilde{M} = U\widetilde{D}V^T$, the new matrix of rank $R$.
    - Set $\mathbf{M}^{(l)}$as : 
    \[
      \mathbf{M}^{(l)} = \begin{cases} X_{ij} \quad \text{for } (i,j) \in \Omega \\
                                       \widetilde{M}_{i,j} \quad \text{for } (i,j) \notin \Omega \end{cases}
      \]
  * **while $||M^{(l)}-M^{(l-1)}||_F> tol$**:
  * Store the error $\sum_{(i,j) \in \Omega} (X_{ij} - M^{(l)}_{ij})^2$ and then repeat this experiment with different values of $R$. Pick the $R$ which gives the lowest error.
    
## 3. Simulation and comparison of CV methods

### 3.1 Simulation dataset description

After the end of [2. Cross-validation on PCA methods] we will be equipped with multiple approaches which enable us to recover the best amount of dimensions needed to represent data "accurately" of a data set. Intuitively, every method must have its different advantages and draw backs.
In order to get a better feeling on how our Cross-Validation methods behave in different circumstances, we will run a simulation study on a variety of synthesized data sets. This allows us to discover the limits of the different methods used and allow us to conclude and recommend different Algorithms with respect to its underlying data. 
As by construction of our Alogrithms every single one of them besides the [2.1 Naïve Approach], the [2.5 Matrix completion method] is only applicable to Multivariate Gaussian distributed data. Therefore we restrict ourselves to this special distribution and create corresponding data in order to get meaningful results. All in all, we consider $12$ different data sets and compare the results of our five algorithms. Furthermore, we only consider centered Gaussian Multivariate Random variables and create three base data sets, on which we infer four kinds of different noises. The first data set is a standard Multivariate Gaussian data set while the second and third base data sets will rely on a random and structured covariance matrix. We denote our base data sets as $D^{i}_0, D^{i}_1$ and $D^{i}_2$ and $i \in \{1,\dots,4\}$. It consists of data $X=\{X^1,\dots,X^n\}$ and noise $\epsilon^{i}$.

\[
D^{i}_0 = X + \epsilon^{i}_p, \text{ where } X^j\sim\mathcal{N}(0, \mathbb{1}_{p\times p}), \forall j = 1,\dots,n\\
D^{i}_1 = X + \epsilon^{i}_p, \text{ where } X^j\sim\mathcal{N}(0, \Sigma_1), \forall j = 1,\dots,n \text{ and } \Sigma_1=M\cdot M^{T}, (M_{ij})_{1\leq i,j\leq p} \stackrel{\text{iid}}{\sim} \mathcal{N}(0,1)\\
D^{i}_2 = X + \epsilon^{i}_p, \text{ where } X^j\sim\mathcal{N}(0, \Sigma_2), \forall j = 1,\dots,n \text{ and } \Sigma_2=M\cdot M^{T}, (M_{ij})_{1\leq i,j\leq p}=\frac{(i-1)p+j}{p}
\]

For the covariance $\Sigma_2$ of $D^{i}_2$ we will get a structured matrix that will emphasize the meaningful position of the last variables. For clarity purposes, we will give an example for $p=3$:

\[
M = 
\frac{1}{9}\begin{pmatrix}
  1 & 2 & 3 \\
  4 & 5 & 6 \\
  7 & 8 & 9 \\
\end{pmatrix}
\implies
\Sigma_2 = \frac{1}{81}\begin{pmatrix}
0.81 & 0.96 & 1.11 \\
0.96 & 1.15 & 1.33 \\
1.11 & 1.33 & 1.56 \\
\end{pmatrix}
\]

In order to have valid comparisons across all of our methods and validate their correctness, we modify $X$. First, we realize that by the random construction of $X$, we can assume that it has full rank $p$. Second, we truncate it to a specific rank $r$, which should be the recommended dimensions of our Algorithms. Due to asymmetry of $X$, we do so by performing a singular value decomposition of $X$ with the built-in `svd()` function. We will set the $p-r$ smallest of its singular values to $0$. This enables us to control the rank of $X$. We realize that $D_2$ is rank $2$ before adding noise. Having adapted $X$, we infer either uniform noise, differing noise or noise that will increase for each variable.

\[
\epsilon^{1}_p\stackrel{\text{iid}}{\sim}\mathcal{N}(0,\sigma^2\mathbb{1}_{p\times p}), \sigma^2=1.5 \\
\epsilon^{2}_p\stackrel{\text{iid}}{\sim}\mathcal{N}(0,\sigma^2\mathbb{1}_{p\times p}), \sigma^2=0.2 \\
\epsilon^{3}_p\stackrel{\text{iid}}{\sim}\mathcal{N}(0, U), U_{ii}\stackrel{\text{iid}}{\sim} \mathcal{U}([0,2]) \text{ and } U_{ij}=0, \text{for } i\neq j \\
\epsilon^{4}_p\stackrel{\text{iid}}{\sim}\mathcal{N}(0, U), U_{ii}= \frac{2i}{p}\text{ and } U_{ij}=0, \text{for } i\neq j
\]

### 3.1 Comparison with respect to noise

### 3.2 Comparison with respect to initial covariance matrix (rank and spectral shape)

## 4. Other PCs number selection methods

### 4.1 Percentage of variance explained method

The percentage of variance explained method is based on the dataset's covariance matrix, and the percentage of variance vs. the total variability (sum of diagonal entries of the original covariance matrix). When computing the PCs for a given dataset $\mathbf x_1,\ldots,\mathbf x_N \in \mathbb{R}^p$, we create an orthonormal basis with a corresponding diagonal covariance matrix with its entries being the eigenvalues of corresponding PCs. As we sequentially construct the components, the eigenvalues $\lambda_1, \lambda_2, ... ,\lambda_r \in \mathbb{R}$ will be a decreasing function. The method of percentage of variance explained is simply to select the first $r$ components such that $\sum_{i=1}^r \lambda_i > \tau V$ for an arbitrary treshold $\tau$ and a total variability $V = \sum_{i=1}^p \lambda_i$.


### 4.2 Scree-plot method (non-automatic)


The Scree-plot is again related to the eigenvalues of the newly created orthonormal basis of PCs. As shown above, the eigenvalues corresponding to the PCs are a decreasing sequence, so we get a graph as below when plotting the PCs index vs the eigenvalue :

```{r, echo=F, out.width='50%', fig.align='center'}
knitr::include_graphics('./Figures/scree_plot_example.png')
```

The Scree-plot method consists of choosing the value of $r$ which is the elbow of the graph as seen above. In this example, the optimal value of $r$ is $3$.


### 4.3 Comparison with CV methods
In regression methods and other similar techniques, the goal of CV is monitoring overfitting and calibrating hyperparameters. If we have a large amount of predictors, we would usually add regularization to the model.
Therefore, we can use CV to tune model hyperparameters (e.g. $\lambda$ in LASSO). PCA also has an important hyperparameter to evaluate: the number of components in the model.  In a common way, many cluster analysis models require to choose the amount of clusters before fitting the model. Viewed from the perspective of matrix factorization, these are very similar problems.


## 5. Conclusion