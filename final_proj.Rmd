---
title: "Final Project"
author: "Emanuele Sorgente - Noah Scheider - Eliott Van Dieren"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Todo List
Final deadline : 22/12 to send

<ol>
<li>Implement Artificially turn the unsupervised problem into a supervised one</li>
<li>Implement Missing data approach -> X must be Gaussian</li>
<li>Implement Modify the KDE approach</li>
<li>Implement Use matrix completion, a.k.a. PCA with missing data</li>
<li>Comparison between those 4 methods on Gaussian data - Emphasize on noise, correlation, hyperparameter r, comparing with elbow plots</li>

<li>Real world data? - Gaussian with (Gaussian) noise</li>

<li>Incorporate "General Feedback for Small Projects" - Manual 11 </li>
</ol>

## 1. Introduction
Which extend do we have to explain the theory? (PCA itself and 4 methods for CV) -> Kinda introduction to CV on PCA, like to read yourself in order to get familiar, meaningful plots, refer to further informations when just scratching them, explain PCA mehtods and difference


When dealing with modern large datasets, the observations' dimensionality often becomes a problem when dealing with interpretability of the dataset, or for vizualisation. The Principal Component Analysis (PCA) is a satatistical technique used to reduce the dimension of the dataset, while trying to keep as much information as possible from the original dataset. For example, PCA is often used to bring the high-dimension dataset to $\mathbb{R}^2$ or $\mathbb{R}^3$ to vizualise the datapoints and identify clusters. It does so by computing Principal components from the dataset and projecting datapoints to this new basis, therefore reduing the dimensionnality of the datapoints.

Mathematically speaking, we can define principal components as an orthonormal basis of directions which sequentially capture the most variance in the data. Practically, the PCs can be found with the following optimization problems for a given centered Gaussian dataset $\mathbf x_1,\ldots,\mathbf x_N \in \mathbb{R}^p$: 
\[
\begin{split}
v_1 &= \underset{v, \| v \|=1}{\mathrm{arg\,max}} \mbox{ } v^\top \widehat{\boldsymbol{\Sigma}} v \\
v_2 &= \underset{v, \| v \|=1, v^\top v_1 = 0}{\mathrm{arg\,max}} v^\top \widehat{\boldsymbol{\Sigma}} v \\
&\;\;\vdots
\end{split}
\]

with $\widehat{\Sigma} = N^{-1}\sum_{n=1}^N \mathbf x_n \mathbf x_n^\top$ being the empirical covariance matrix of the dataset. More information about Principal component analysis can be found in [Jolliffe, 2002](https://link.springer.com/book/10.1007/b98835) or directly on [wiki](https://en.wikipedia.org/wiki/Principal_component_analysis)


This project will focus on finding the optimal No. of PCs ($r$), which dictates a trade-off between the complexity of the model and the interpretability of the dataset. In fact, the higher $r$, the higher the explained variance, but less interpretable the dataset. To optimally choose the No. of components, several techniques have been created, such as the method of percentage of variance explained, using the Scree-plot or Cross-Validation for PCA.

The percentage of variance explained method is based on the covariance matrix of our datased, and the percentage of variance vs the total variabillity (sum of diagonal entries of the original covariance matrix). When computing the PCs, we create an orthonormal basis, with a corresponding diagonal covariance matrix. Each diagonal entry is the eigenvalue related to each components. As we sequentially construct the components, the sequence of eigenvalues will decrease. The technique of percentage of variance explained is simply to select the first $r$ components such that $\sum_{i=1}^{r} \lambda_i > \alpha $ for an arbitrary $\alpha$.

Cross-validation for PCA is used for 

### 1.3 Simulation dataset description

## 2. Cross-validation on PCA methods

### 2.1 Artificially turn the unsupervised problem into a supervised one

### 2.2 Missing data approach

## 3. Comparison of CV methods

## 4. Other dimension-reduction techniques and comparison with CV

### 4.1 Percentage of variance explained method

### 4.2 Scree-plot method (non-automatic)

### 4.3 Comparison with CV methods

## 5. Conclusion