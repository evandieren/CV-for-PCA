---
title: "Final Project"
author: "Emanuele Sorgente - Noah Scheider - Eliott Van Dieren"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(mvtnorm)
```

## Todo List
Final deadline : 22/12 to send

<ol>
<li>~~To which extend do we have to explain the theory? (PCA itself and 4 methods for CV) -> Kinda introduction to CV on PCA, like to read yourself in order to get familiar, meaningful plots, refer to further informations when just scratching them, explain PCA mehtods and difference~~</li>
<li>~~Implement Artificially turn the unsupervised problem into a supervised one~~</li>
<li>~~Implement Missing data approach (X must be Gaussian)~~</li>
<li>~~Implement Modify the KDE approach~~</li>
<li>~~Implement Use matrix completion, a.k.a. PCA with missing data~~</li>
<li>~~Comparison between those 4 methods on Gaussian data - Emphasize on noise, correlation, hyperparameter r, comparing with elbow plots~~</li>
<li>~~Real world data? - Gaussian with (Gaussian) noise~~ (no accessible multivariate data set on real world data)</li>
</ol>

**Work for 2.12-4.12:**

<span style="color:brown">Emanuele</span>
<span style="color:orange">Noah</span>
<span style="color:gray">Eliott</span>

* Find a multivariate Gaussian Data set <span style="color:brown">Emanuele</span>
* ~~Check Method0 - WrongPCA again <span style="color:orange">Noah</span>~~ DONE
* ~~Check Method1 - WrongPCAImproved again <span style="color:orange">Noah</span>~~ DONE
* Check Method2 - MissingData again <span style="color:gray">Eliott</span>
* Check Method3 - KDEApproach again <span style="color:brown">Emanuele</span>
* ~~Check Method4 - MatrixCompletion <span style="color:gray">Eliott</span>~~ A verifier avec le prof car pas super clair sur certains points
* Run simulation study on synthesized data (- Emphasize on noise, correlation, hyperparameter r, comparing with elbow plots, use the same generated data for the different methods) - document along the way <span style="color:orange">Noah</span>, <span style="color:gray">Eliott</span>

* Report:
  
  * For **each** method explanation, pseudo-code, simulation & documentation, conclusion <span style="color:orange">Noah</span>, <span style="color:gray">Eliott</span>, <span style="color:brown">Emanuele</span>
  * Extend [1.3 Simulation dataset description] <span style="color:gray">Eliott</span>
  * Overall conclusion

  
* Polishing:

  * Find multivariate gaussian real data
    * Implement and comment on it
    * Add a data section (1.4) for the real data set
  * Consider and incorporate "General Feedback for Small Projects" - Manual 11 (Copied to our github)
  * Read through other parts and note down suggestions and **any** uncertainties
  * Ask Masàk next week about those


* Questions:
  
  * Missing Data: Singularity issues in l_comp?
  * Missing Data: Q or l_comp for convergence
  * Missing Data: C is sigma from previous loop?
  * Missing Data: mu_l or mu_l-1?
  * KDE-Approach: Correct?
  * MatrixCompletion: Tolerance and CV
  * MatrixCompletion: To what extent does noise influnence -_-
 


## 1. Introduction

When dealing with modern large datasets, the observations' dimensionality often becomes a problem when dealing with interpretability of the dataset, or for its vizualisation, especially in a data exploration phase. The Principal Component Analysis (PCA) is a statistical technique used to reduce the dimension of the dataset, while trying to keep as much information as possible from the original dataset. For example, PCA is often used to bring a high-dimension dataset to $\mathbb{R}^2$ or $\mathbb{R}^3$ to vizualise its datapoints and identify clusters. It does so by computing Principal components from the dataset and then projecting datapoints to this new basis.

Mathematically speaking, we can define principal components as an orthonormal basis of unit vectors which sequentially capture the most variance in the data. Practically, the PCs can be found with the following optimization problems for a given centered Gaussian dataset $\mathbf x_1,\ldots,\mathbf x_N \in \mathbb{R}^p$: 
\[
\begin{split}
v_1 &= \underset{v, \| v \|=1}{\mathrm{arg\,max}} \mbox{ } v^\top \widehat{\boldsymbol{\Sigma}} v \\
v_2 &= \underset{v, \| v \|=1, v^\top v_1 = 0}{\mathrm{arg\,max}} v^\top \widehat{\boldsymbol{\Sigma}} v \\
&\;\;\vdots
\end{split}
\]

with $\widehat{\Sigma} = N^{-1}\sum_{n=1}^N \mathbf x_n \mathbf x_n^\top \in \mathbb{R}^{p\times p}$ being the empirical covariance matrix of the dataset. More information about Principal component analysis can be found in [Jolliffe, 2002](https://link.springer.com/book/10.1007/b98835) or directly on [wiki](https://en.wikipedia.org/wiki/Principal_component_analysis)

```{r, echo=F, out.width='25%', fig.align='center'}
knitr::include_graphics('./Figures/PCA_p_2_example.png')
```


This project will focus on finding the optimal number of principal components ($r$), which dictates a trade-off between the model's complexity and its interpretability. In fact, higher the $r$, higher the explained variance, but less interpretable the dataset. To optimally choose the number of PCs, several techniques have been created, such as the method of percentage of variance explained, using the Scree-plot or Cross-validation for PCA (described in [2. Cross-validation on PCA methods]).

The percentage of variance explained method is based on the dataset's covariance matrix, and the percentage of variance vs. the total variability (sum of diagonal entries of the original covariance matrix). When computing the PCs for a given dataset $\mathbf x_1,\ldots,\mathbf x_N \in \mathbb{R}^p$, we create an orthonormal basis with a corresponding diagonal covariance matrix with its entries being the eigenvalues of corresponding PCs. As we sequentially construct the components, the eigenvalues $\lambda_1, \lambda_2, ... ,\lambda_r \in \mathbb{R}$ will be a decreasing function. The method of percentage of variance explained is simply to select the first $r$ components such that $\sum_{i=1}^r \lambda_i > \tau V$ for an arbitrary treshold $\tau$ and a total variability $V = \sum_{i=1}^p \lambda_i$.

The Scree-plot is again related to the eigenvalues of the newly created orthonormal basis of PCs. As shown above, the eigenvalues corresponding to the PCs are a decreasing sequence, so we get a graph as below when plotting the PCs index vs the eigenvalue :
```{r, echo=F, out.width='50%', fig.align='center'}
knitr::include_graphics('./Figures/scree_plot_example.png')
```

The Scree-plot method consists of choosing the value of $r$ which is the elbow of the graph as seen above. In this example, the optimal value of $r$ is $3$.

### 1.3 Simulation dataset description

After the end of [2. Cross-validation on PCA methods] we will be equipped with multiple approaches which enable us to recover the best amount of dimensions needed to represent data "accurately" of a data set. Intuitively, every method must have its different advantages and draw backs.
In order to get a better feeling on how our Cross-Validation methods behave in different circumstances, we will run a simulation study on a variety of synthesized data sets. This allows us to discover the limits of the different methods used and allow us to conclude and recommend different Algorithms with respect to its underlying data. 
As by construction of our Alogrithms every single one of them besides the [2.1 Naïve Approach], the [2.5 Matrix completion method] is only applicable to Multivariate Gaussian distributed data. Therefore we restrict ourselves to this special distribution and create corresponding data in order to get meaningful results. All in all, we consider twelve different data sets and compare the results of our $5$ Algorithms. Furthermore, we only consider centered Gaussian Multivariate Random variables with mean $0$. We create three base data sets, on which we infer for kinds of different noises. In order to compare those we start with a standard Gaussian data set. The second and third base data sets will rely on a random and structured covariance matrix. We denote our base data sets as $D^{i}_0, D^{i}_1$ and $D^{i}_2$ and $i \in \{1,\dots,4\}$. It consists of data $X=\{X^1,\dots,X^n\}$ and noise $\sigma$.

\[
D^{i}_0 = X + \epsilon^{i}_p, \text{ where } X^j\sim\mathcal{N}(0, \mathbb{1}_{p\times p}), \forall j = 1,\dots,n\\
D^{i}_1 = X + \epsilon^{i}_p, \text{ where } X^j\sim\mathcal{N}(0, \Sigma_1), \forall j = 1,\dots,n \text{ and } \Sigma_1=M\cdot M^{T}, (M_{ij})_{1\leq i,j\leq p} \stackrel{\text{iid}}{\sim} \mathcal{N}(0,1)\\
D^{i}_2 = X + \epsilon^{i}_p, \text{ where } X^j\sim\mathcal{N}(0, \Sigma_2), \forall j = 1,\dots,n \text{ and } \Sigma_2=M\cdot M^{T}, (M_{ij})_{1\leq i,j\leq p}=\frac{(i-1)p+j}{p}
\]

For the covariance $\Sigma_2$ of $D^{i}_2$ we will get a structured the matrix that will emphasize the meaningful position of the last variables. To make this more clear, we will give an example for $p=3$:

\[
M = 
\frac{1}{9}\begin{pmatrix}
  1 & 2 & 3 \\
  4 & 5 & 6 \\
  7 & 8 & 9 \\
\end{pmatrix}
\implies
\Sigma_2 = \frac{1}{81}\begin{pmatrix}
0.81 & 0.96 & 1.11 \\
0.96 & 1.15 & 1.33 \\
1.11 & 1.33 & 1.56 \\
\end{pmatrix}
\]

In order to have valid comparisons across all of our methods and validate their correctness, we modify $X$. First, we realize that by the random construction of $X$, we can assume that it has full rank $p$. Second, we truncate it to a specific rank $r$, which should be the recommended dimensions of our Algorithms. Due to asymmetry of $X$, we do so by performing a singular value decomposition of $X$ with the built-in `svd()` function. We will set the $p-r$ smallest of its singular to $0$. This enables us to control the rank of $X$. We realize that $D_2$ is rank $2$ before adding noise. Having adapted $X$, we infer either uniform noise, differing noise or noise that will increase for each variable.

\[
\epsilon^{1}_p\stackrel{\text{iid}}{\sim}\mathcal{N}(0,\sigma^2\mathbb{1}_{p\times p}), \sigma^2=1.5 \\
\epsilon^{2}_p\stackrel{\text{iid}}{\sim}\mathcal{N}(0,\sigma^2\mathbb{1}_{p\times p}), \sigma^2=0.2 \\
\epsilon^{3}_p\stackrel{\text{iid}}{\sim}\mathcal{N}(0, U), U_{ii}\stackrel{\text{iid}}{\sim} \mathcal{U}([0,2]) \text{ and } U_{ij}=0, \text{for } i\neq j \\
\epsilon^{4}_p\stackrel{\text{iid}}{\sim}\mathcal{N}(0, U), U_{ii}= \frac{2i}{p}\text{ and } U_{ij}=0, \text{for } i\neq j
\]

## 2. Cross-validation on PCA methods

```{r, echo=F, eval=F}
# Dataframe as input
df <- c()
# Create folds for Cross-validation
df_df <- data.frame("obs1"=c(5,43,2,3,1,4,8,4,5,9), "obs2"=c(5,6,7,3,9,9,12,1,5,8), "obs3"=c(1,2,5,3,4,5,6,1,5,7), "obs4"=c(1,7,6,3,4,5,6,1,7,10))
df <- as.matrix(df_df)
n <- dim(df)[1]
p <- dim(df)[2]
index <- c(1:n)
K <- 3
samples <- vector("list", K)
for (i in 1:K) {
  if (i < K) {
    fold <- sample(index, n/K)
    samples[[i]] <- fold
    #print(which(fold %in% index))
    index <- index[-which(index %in% fold)]
  }
  else{
    samples[[i]] <- index
  }
}
# Index can now be used to slice df in every step of Cross-validation

# How is r set? r < p
r <- 3
# Sample randomly r columns
trunc <- sample(1:p, p-r)
```

### 2.1 Naïve Approach

A widely used approach for Cross-validation in PCA is a method that has the right intentions but fails at a critical point. Imaging sliding down slide with no end. Gravity would make that an endless ride. Same concept is happening here. But due to computational issues the endlessness is adapted to finite. This is due to the fact that this Naïve Approach does not take into account any variance. Let the algorithm as presented in [Notes 06 - T. Masak](https://htmlpreview.github.io/?https://github.com/TMasak/StatComp/blob/master/Notes/06_CV.html#no.-of-component-selection-for-pca) speak for itself. We start with the usual conventions by denoting $X \in \mathbb{R}^{n \times p}$ our data with $n$ observations and $p$ variables:

  * split data into $K$ folds $J_1,\ldots,J_K$
  * **for** $k=1,\ldots,K$:
    - solve $\widehat{\mathbf{L}} = \underset{\mathrm{rank}(\mathbf L) = r}{\mathrm{arg\,min}} \|\mathbf X[J_k^c,] -\mathbf L \|_2^2$
    - calculate $Err_k(r) = \frac{1}{|J_k|}\sum_{m \in J_k} \| x^m - P_{\widehat{\mathbf{L}}} x^m \|_2^2$
  * **end for**
  * choose $\widehat{r} = \underset{r}{\mathrm{arg\,min}} \sum_{k=1}^K Err_k(r)$

A numerical method to solve the least square problem in the top of the algorithm would be to compute the SVD decomposition of $\mathbf X[J_k^c,]$ and truncate the $p-r$ smallest singular values. In order to estimate $\mathbf{x^{m}}, \forall m \in J_k$ with a lower dimensional version we project it by the matrix $P_{\mathbf{\widehat{L}}}$. Where does this matrix come from and how is it computable? To resolve this we took advantage of the R built-in function `prcomp()` the returns us among other objects a basis for the dimension of $\widehat{\mathbf L}$. The promised issue occurs when computing the our error measure without further to-do. Thus the error obviously shrinks when $\widehat{\mathbf L}$ is allowed to have higher dimensions and atteins $0$ when its number of dimensions equals those of the initial data set $\mathbf{X}$.

Indeed



### 2.2 Artificially turn the unsupervised problem into a supervised one

We now want to "correct" the [2.1 Naïve Approach] and artificially induce a Bias-Variance trade-off. As described in [Notes 06 - T. Masak](https://htmlpreview.github.io/?https://github.com/TMasak/StatComp/blob/master/Notes/06_CV.html#no.-of-component-selection-for-pca), we implement this method as below. Let again $\mathbf{X} \in \mathbb{R}^{n \times p}$ be our Gaussian data with $n$ observations and $p$ variables:

  * split data into $K$ folds $J_1,\ldots,J_K$
  * **for** $k=1,\ldots,K$:
    - Compute $\hat{\mu}$ and $\hat{\Sigma} := \hat{\Sigma}_r$ (the empirical estimator truncated to rank r) on $\mathbf{X}\setminus J_k$.
    - Split data points $\mathbf{x}^m\in \mathbb{R}^{p}, m \in J_k$ into a "missing" part $\mathbf{x}_{miss}$ that will be used for validation and an "observed" part $\mathbf{x}_{obs}$ in order to estimate its counterpart$
    - Predict the missing part from the observed part using the $\hat{\mu}$ and $\hat{\Sigma}_r$
    - That is compute $\hat{\mathbf{x}}^m_{miss} = \mathbb{E}_{\hat{\mu},\hat{\Sigma}_r}[\mathbf{X}^m_{miss}|\mathbf{X}^m_{obs}=\mathbf{x}^m_{obs}] = \hat{\mu}_{miss} + \hat{\Sigma}_{miss,obs}\hat{\Sigma}^+_{obs,obs}(\mathbf{x}^m_{obs}-\hat{\mu}_{obs})$, where $\hat{\Sigma}^+_{obs,obs}$ is considered the Pseudoinverse of $\hat{\Sigma}_{obs,obs}$ and $\hat{\mu}_{miss},\hat{\mu}_{obs}$ is $\hat{\mu}$ split according to the "missed" and "observed" part
  * **end for**
  * choose $\widehat{r} = \underset{r}{\mathrm{arg\,min}} \sum_{k=1}^K Err_k(r)$, where for instance $Err_k(r)$ can be considered as the MSE for every fold $J_k$, e.g. $Err_k(r)=\frac{1}{|J_k|}\sum_{m\in J_k} \|\hat{\mathbf{x}}^m_{miss}-\mathbf{x}^m_{miss}\|^2_2$

Before continuing with our simulations we want to emphasize on some more and some less obvious clues in order to avoid any confusion. When we truncate $\hat{\Sigma}$ to $\hat{\Sigma}_r$ in every fold, we actually take its SVD decomposition and "cut off" its $p-r$ smallest singular values. Hence, we reduce the rank of the covariance of $\mathbf{X}$, but we preserve its dimension which allows us to estimate $\mathbf{x}^m_{miss}$ in the later part of the algorithm. When splitting the observations in every iteration of the for loop, it naturally comes to mind if we should always take the same or a different (random) split. We went for the first possibility as that additional random factor would be negligible by the SLLN. Nevertheless, it makes sense to keep two equally sized parts in order to have a proper estimation basis without overestimating.
When we estimate $\hat{\mathbf{x}}^m_{miss}$ we use a specific formula that emerges from the law of our data set $\mathbf{X}$. Hence, it is only applicable in this form to Gaussian distributed observations $\mathbf{x} \in \mathbb{R}^p$. The estimation also makes use of the Pseudoinverse of $\hat{\Sigma}_{obs,obs}$ denoted by $\hat{\Sigma}^+_{obs,obs}$. We use this convention in order to avoid singularity conflicts. The Pseudoinverse is easily callable in R by the function `ginv()` embedded in the `MASS` package.

```{r, echo=F, eval=F}
# Cross-validation: "Artificially turn the unsupervised problem into a supervised one"

# Split data into missing and observed data on jth component
# e.g. split missing and observed data in half, or split variables randomly
# split <- sample(1:p, ceiling(ratio)), ratio < r as data is truncated to rank r

split <- 1:ceiling(r/2)

mse1 <- rep(0, K)

for (k in 1:K) {
  l1 <- length(samples[[k]])
  
  df_k <- df[-samples[[k]],]
  mu <- colMeans(df_k)
  mu_trunc <- mu[-trunc]
  sigma <- cov(df_k)
  sigma_trunc <- sigma[-trunc,-trunc]
  
  df_k_fold <- df[samples[[k]],]
  df_k_fold_trunc <- df_k_fold[,-trunc]
  df_k_fold_miss <- as.matrix(df_k_fold_trunc[,split])
  df_k_fold_obs <- as.matrix(df_k_fold_trunc[,-split])
  
  # Estimate est_x_miss for df_k_fold_miss
  mu_miss <- mu_trunc[split]
  mu_obs <- mu_trunc[-split]
  sigma_miss_obs <- sigma_trunc[split, -split]
  sigma_obs_obs <- sigma_trunc[-split, -split]

  est_x_miss <- lapply(1:l1, function(n){mu_miss+sigma_miss_obs%*%solve(sigma_obs_obs)%*%(df_k_fold_obs[n,]-mu_obs)})
  
  # Error of estimated and true missing observation
  for(s in 1:l1){
    mse1[k] <- mse1[k] + norm(est_x_miss[[s]]-df_k_fold_miss[s,], type = "2")^2/l1/K
  }
}
```
### 2.3 Missing data approach

Here it is assumed that the rows $X_{n} \in \mathbb{R}^p, n=1,…,N$ of the data matrix $X=(X_{n,j})^{N,p}_{n,j=1}$ are i.i.d. realizations of a Gaussian random variable $X$. That is, the rows have with a mean $\mu \in \mathbb{R}^p$ and a covariance $\Sigma \in \mathbb{R}^{p \times p}$. We will assume that $rank(\Sigma)=r$ for $r=1,…,R$, and compare how well different values of $r$ fit the data. As described in [Notes 06 - T. Masak](https://htmlpreview.github.io/?https://github.com/TMasak/StatComp/blob/master/Notes/06_CV.html#no.-of-component-selection-for-pca), this approach is based on computing the estimators $\hat{\Sigma}$ and $\hat{\mu}$ on observed data by the EM algorithm and the set of missed and observed data is randomly selected for each fold $K_i$. Let us now describe the EM algorithm used in our case and then describe the Missing data approach algorithm.

Let $\tilde{X}$ be the dataset containing missing values randomly placed on the original $N \times p$ dataset. The EM algorithm works as follows :

  * Initialize $\hat{\mu} = \frac{1}{N}\sum_{n=1}^N(\tilde{X}^{obs}_{n})$ and $\hat{\Sigma} = I_{p\times p}$, where $\tilde{X}_{obs}$ is the dataset restricted to observed values.
  * Set $\hat{\Sigma}^{old} = 6\hat{\Sigma}$, and $tol = 0.01$
  * **while** $(||\hat{\Sigma}-\hat{\Sigma}^{old}||_F > tol*||\hat{\Sigma}^{old}||_F)$ where $||.||_F$ is the Frobenius norm
    * Set $\hat{\Sigma}^{old} = \hat{\Sigma}$
    * Estimate $\tilde{X}_{miss}^{n}$ by a linear estimator : $\tilde{X}_{miss}^n = \hat{\mu}_{miss} + \hat{\Sigma}_{miss,obs} \hat{\Sigma}_{obs}^{\dagger}(\tilde{X}_{obs}^n-\hat{\mu}_{obs})$
    * Define $\hat{X}$ being $\tilde{X}$ with missing values filled by the linear estimator above.
    * Set $\hat{\mu} = mean(\hat{X})$, $\hat{\Sigma} = \frac{1}{N-1}\sum_{n=1}^N (\hat{X}_i-\hat{\mu})(\hat{X}_i-\hat{\mu})^T$, also known as the two MLE estimators
  * **end while**

This method slightly differs from the one presented in [Notes 05 - T. Masak](https://htmlpreview.github.io/?https://github.com/TMasak/StatComp/blob/master/Notes/05_EM.html), mainly due to poor results when estimating $\hat{\Sigma}$. Another matrix approach for the EM algorithm was also tried from Machine Learning, Murphy but was also unsuccessfull when predicting $\hat{\Sigma}$.

Below is the pseudo-code for the Missing Data method :

  * split the dataset in $K$ folds by randomly selecting indices from the bivariate index set $J := \{(n,j) | n = 1,...,N, \quad, j=1,...,p\}$.
  * **for** $k=1,\ldots,K$:
    - Compute the bivariate set of missed data $\Omega$
    - Compute $\hat{\mu}$ and $\hat{\Sigma}$ by the EM algorithm with the dataset $X$ and $\Omega$ as inputs
    - Truncate $\hat{\Sigma}$ to a rank $r$ matrix $\hat{\Sigma}_r$ by eigenvalue decomposition, + ADD NOISE
    - Reuse the missed data $\Omega$ to split the data points $\mathbf{x}^m\in \mathbb{R}^{p}, m \in J_k$ into a "missing" part $\mathbf{x}_{miss}$ that will be used for validation and an "observed" part $\mathbf{x}_{obs}$ in order to estimate its counterpart$
    - Predict the missing part from the observed part using the $\hat{\mu}$ and $\hat{\Sigma}_r$
    - That is compute $\hat{\mathbf{x}}^m_{miss} = \mathbb{E}_{\hat{\mu},\hat{\Sigma}_r}[\mathbf{X}^m_{miss}|\mathbf{X}^m_{obs}=\mathbf{x}^m_{obs}] = \hat{\mu}_{miss} + \hat{\Sigma}_{miss,obs}\hat{\Sigma}^\dagger_{obs}(\mathbf{x}^m_{obs}-\hat{\mu}_{obs})$, where $\hat{\Sigma}^\dagger_{obs}$ is considered the Pseudoinverse of $\hat{\Sigma}_{obs,obs}$ and $\hat{\mu}_{miss},\hat{\mu}_{obs}$ is $\hat{\mu}$ split according to the "missed" and "observed" part
  * **end for**   
  * choose $\widehat{r} = \underset{r}{\mathrm{arg\,min}} \sum_{k=1}^K Err_k(r)$, where for instance $Err_k(r)$ can be considered as the MSE for every fold $J_k$, e.g. $Err_k(r)=\frac{1}{|J_k|}\sum_{m\in J_k}



```{r echo=F, eval=F}

# Cross-validation: "Missing Data Approach"

# Split data into missing and observed data on jth component
# e.g. split missing and observed data in half, or split variables randomly
# splitting the data can be done for all variables regardless of r
# split <- sample(1:p, ceiling(ratio)), ratio !< r
split <- 1:ceiling(r/2)

mse2 <- rep(0, K)

for (k in 1:K) {
  l1 <- length(samples[[k]])
  
  df_k <- df[-samples[[k]],]
  mu <- colMeans(df_k)
  svd_sigma <- svd(cov(df_k))
  sigma_trunc <- svd_sigma$u[,-trunc] %*% diag(svd_sigma$d[-trunc]) %*% t(svd_sigma$u[,-trunc])
  cov(df_k)
  
  df_k_fold <- df[samples[[k]],]
  df_k_fold_miss <- as.matrix(df_k_fold[,split])
  df_k_fold_obs <- as.matrix(df_k_fold[,-split])
  
  # Estimate est_x_miss for df_k_fold_miss
  mu_miss <- mu[split]
  mu_obs <- mu[-split]
  sigma_miss_obs <- sigma_trunc[split, -split]
  sigma_obs_obs <- sigma_trunc[-split, -split]
  
  est_x_miss <- lapply(1:l1, function(n){mu_miss+sigma_miss_obs%*%solve(sigma_obs_obs)%*%(df_k_fold_obs[n,]-mu_obs)})
  
  # Error of estimated and true missing observation
  for(s in 1:l1){
    mse2[k] <- mse2[k] + norm(est_x_miss[[s]]-df_k_fold_miss[s,], type = "2")^2/l1/K
  }
  
}

mse1
mse2
```


### 2.4 KDE modified approach
This approach is a modfication of the KDE  method. As indicated in [Notes 06 - T. Masak](https://htmlpreview.github.io/?https://github.com/TMasak/StatComp/blob/master/Notes/06_CV.html#no.-of-component-selection-for-pca) the goal is to minimize the rank $R$ in

\[
\mathbb{E} \| \widehat{C}_R - C \|_2^2 = \mathbb{E}\| \widehat{C}_R \| - 2 \mathbb{E} \langle \widehat{C}_R, C \rangle + \| C \|_2^2
\]

with $\widehat{C}_R$ being the truncated estimator having rank $R$.
Assuming that the mean is zero we, in our modification we'll get:
\[
\mathbb{E} \langle \widehat{C}_R^{(-n)}, X_n X_n^\top \rangle = \langle \underbrace{\mathbb{E}[ \widehat{C}_R^{(-n)}}_{\approx \mathbb{E} \widehat{C}_R}], \underbrace{\mathbb{E} [X_n X_n^\top}_{ = C}] \rangle \approx \mathbb{E} \langle \widehat{C}_R , X_n X_n^\top \rangle
\]

Therefore, since the truncated covariance estimator isn't linear we obtain an approximation. Based on this strategy we'll choose the rank $R$ to minimize our CV. Computing this approximation and averaging it for every observation, aswell as neglecting the $\| C \|_2^2$ (constant error) will produce a useful measure error. Overall, we implement this method as follows for every rank $R$:

* Estimate Covariance $\widehat{C}$ from given data $X$
* Decompose $\widehat{C}$ and set $p-R$ smallest eigenvalues to $0$
* Compute Frobenius-norm $\|\cdot\|=\|\cdot\|_F$
* **for** $j$ in $1,\dots,n$:
  - calculate $\widehat{C}_R^{(-j)}$ by estimating the covariance of $X^{(-j)}$, $j$ being the missing observation in $X$
  - reduce $\|\widehat{C}_R\|_F$ by $\frac{2}{n}\widehat{C}_R\cdot\widehat{C}_R^{(-j)}$
* **end**
* Choose $R$ according to the minimal error

Stepping into the for-loop we realize that it actually resembles a Leave-One-Out Cross-validation. In this Method we are trading off high computational cost against a stable solution. 



```{r echo=F, eval=F}

#Cross Validation: KDE Modified Approach

# args: X matrix containing data
# returns: error measure of truncated covariance to the real covariance (Frobenius norm)

p <- ncol(X)
n <- nrow(X)
mse4 <- rep(0,p)

for (r in 1:p) {
  eigen_sigma <- eigen(cov(X))
  eigen_sigma$values[-(1:r)] <- 0
  sigma_trunc <- eigen_sigma$vectors %*% diag(eigen_sigma$values) %*% t(eigen_sigma$vectors)
  
  mse4[r] <- mse4[r] + norm(sigma_trunc,type="F")^2
  
  for (i in 1:n){
    eigen_sigma_n <- eigen(cov(X[-i,]))
    eigen_sigma_n$values[-(1:r)] <- 0
    sigma_trunc_n <- eigen_sigma_n$vectors %*% diag(eigen_sigma_n$values) %*% t(eigen_sigma_n$vectors)
    mse4[r] <- mse4[r] - 2/n*sum(sigma_trunc_n*(X[i,]%*%t(X[i,])))
  }
  }
mse4

```

### 2.5 Matrix completion method

This final methods is somehow a bit more general, as we do not need to assume any probability distribution of the data $X \in \mathbb{R}^{N\times p}$, but just that $X$ has a finite rank. Indeed, for a given data matrix $X$, we assume that a portion of this matrix is missed and that the remaining elements are observed. Let $\Omega$ be a bivariate index set for the observed data in $X$. The matrix completion method consists of finding a matrix $\mathbf M$ of rank $R$ such that :

\[
    \mathrm{arg \, min}_{\mathbf{M}=(M_{ij})_{i,j=1}^{N \times p}} \sum_{(i,j) \in \Omega} (X_{ij} - M_{ij})^2 \quad \mathrm{s.t.} \quad \mathrm{rank}(\mathbf M)=R
    \]

This optimization method gives us the best matrix $\mathbf M$ with rank $R$ with respect to least-square error on the observed data. However, finding the solution of such a optimization problem is $NP$-hard, so one can use the following iterative algorithm to find local best candidates for the matrix $\mathbb M$ :


  * Select an initial candidate $\mathbf{M}^{(0)}$ (here $\mathbf{M}^{(0)} = X$)
  * Set $l=1$
  * **do**
    - Compute the SVD of $\mathbf{M}^{(l-1)}$, i.e. $\mathbf{M}^{(l-1)} = UDV^T$
    - Copy $D$ into $\widetilde{D}$ and set the $p-R$ diagonal elements of $\widetilde{D}$ to zero (truncating to rank $R$)
    - Set $\widetilde{M} = U\widetilde{D}V^T$, the new matrix of rank $R$.
    - Set $\mathbf{M}^{(l)}$as : 
    \[
      \mathbf{M}^{(l)} = \begin{cases} X_{ij} \quad \text{for } (i,j) \in \Omega \\
                                       \widetilde{M}_{i,j} \quad \text{for } (i,j) \notin \Omega \end{cases}
      \]
  * **while $||M^{(l)}-M^{(l-1)}||_2> tol$**:
  * Store the error $\sum_{(i,j) \in \Omega} (X_{ij} - M^{(l)}_{ij})^2$ and then repeat this experiment with different values of $R$. Pick the $R$ which gives the lowest error.
    
## 3. Comparison of CV methods

## 4. Other dimension-reduction techniques and comparison with CV

### 4.1 Percentage of variance explained method

### 4.2 Scree-plot method (non-automatic)

### 4.3 Comparison with CV methods
In regression methods and other similar techniques, the goal of CV is monitoring overfitting and calibrating hyperparameters. If we have a large amount of predictors, we would usually add regularization to the model.
Therefore, we can use CV to tune model hyperparameters (e.g. $\lambda$ in LASSO). PCA also has an important hyperparameter to evaluate: the number of components in the model.  In a common way, many cluster analysis models require to choose the amount of clusters before fitting the model. Viewed from the perspective of matrix factorization, these are very similar problems.


## 5. Conclusion