---
title: "Final Project"
author: "Emanuele Sorgente - Noah Scheider - Eliott Van Dieren"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(mvtnorm)
```

## Todo List
Final deadline : 22/12 to send

<ol>
<li>~~To which extend do we have to explain the theory? (PCA itself and 4 methods for CV) -> Kinda introduction to CV on PCA, like to read yourself in order to get familiar, meaningful plots, refer to further informations when just scratching them, explain PCA mehtods and difference~~</li>
<li>~~Implement Artificially turn the unsupervised problem into a supervised one~~</li>
<li>~~Implement Missing data approach (X must be Gaussian)~~</li>
<li>~~Implement Modify the KDE approach~~</li>
<li>~~Implement Use matrix completion, a.k.a. PCA with missing data~~</li>
<li>~~Comparison between those 4 methods on Gaussian data - Emphasize on noise, correlation, hyperparameter r, comparing with elbow plots~~</li>
<li>~~Real world data? - Gaussian with (Gaussian) noise~~ (no accessible multivariate data set on real world data)</li>
</ol>

**Work for 2.12-4.12:**

<span style="color:brown">Emanuele</span>
<span style="color:orange">Noah</span>
<span style="color:gray">Eliott</span>

* Find a multivariate Gaussian Data set <span style="color:brown">Emanuele</span>
* ~~Check Method0 - WrongPCA again <span style="color:orange">Noah</span>~~ DONE
* ~~Check Method1 - WrongPCAImproved again <span style="color:orange">Noah</span>~~ DONE
* Check Method2 - MissingData again <span style="color:gray">Eliott</span>
* Check Method3 - KDEApproach again <span style="color:brown">Emanuele</span>
* ~~Check Method4 - MatrixCompletion <span style="color:gray">Eliott</span>~~ A verifier avec le prof car pas super clair sur certains points
* Run simulation study on synthesized data (- Emphasize on noise, correlation, hyperparameter r, comparing with elbow plots, use the same generated data for the different methods) - document along the way <span style="color:orange">Noah</span>, <span style="color:gray">Eliott</span>

* Report:
  
  * For **each** method explanation, pseudo-code, simulation & documentation, conclusion <span style="color:orange">Noah</span>, <span style="color:gray">Eliott</span>, <span style="color:brown">Emanuele</span>
  * Extend [1.3 Simulation dataset description] <span style="color:gray">Eliott</span>
  * Overall conclusion

  
* Polishing:

  * Find multivariate gaussian real data
    * Implement and comment on it
    * Add a data section (1.4) for the real data set
  * Consider and incorporate "General Feedback for Small Projects" - Manual 11 (Copied to our github)
  * Read through other parts and note down suggestions and **any** uncertainties
  * Ask Masàk next week about those


* Questions:
  
  * Missing Data: Singularity issues in l_comp?
  * Missing Data: Q or l_comp for convergence
  * Missing Data: C is sigma from previous loop?
  * Missing Data: mu_l or mu_l-1?
  * KDE-Approach: Correct?
  * MatrixCompletion: Tolerance and CV
  * MatrixCompletion: To what extent does noise influnence -_-
 


## 1. Introduction

When dealing with modern large datasets, the observations' dimensionality often becomes a problem when dealing with interpretability of the dataset, or for its vizualisation, especially in a data exploration phase. The Principal Component Analysis (PCA) is a statistical technique used to reduce the dimension of the dataset, while trying to keep as much information as possible from the original dataset. For example, PCA is often used to bring a high-dimension dataset to $\mathbb{R}^2$ or $\mathbb{R}^3$ to vizualise its datapoints and identify clusters. It does so by computing Principal components from the dataset and then projecting datapoints to this new basis.

Mathematically speaking, we can define principal components as an orthonormal basis of unit vectors which sequentially capture the most variance in the data. Practically, the PCs can be found with the following optimization problems for a given centered Gaussian dataset $\mathbf x_1,\ldots,\mathbf x_N \in \mathbb{R}^p$: 
\[
\begin{split}
v_1 &= \underset{v, \| v \|=1}{\mathrm{arg\,max}} \mbox{ } v^\top \widehat{\boldsymbol{\Sigma}} v \\
v_2 &= \underset{v, \| v \|=1, v^\top v_1 = 0}{\mathrm{arg\,max}} v^\top \widehat{\boldsymbol{\Sigma}} v \\
&\;\;\vdots
\end{split}
\]

with $\widehat{\Sigma} = N^{-1}\sum_{n=1}^N \mathbf x_n \mathbf x_n^\top \in \mathbb{R}^{p\times p}$ being the empirical covariance matrix of the dataset. More information about Principal component analysis can be found in [Jolliffe, 2002](https://link.springer.com/book/10.1007/b98835) or directly on [wiki](https://en.wikipedia.org/wiki/Principal_component_analysis)

```{r, echo=F, out.width='25%', fig.align='center'}
knitr::include_graphics('./Figures/PCA_p_2_example.png')
```


This project will focus on finding the optimal number of principal components ($r$), which dictates a trade-off between the model's complexity and its interpretability. In fact, higher the $r$, higher the explained variance, but less interpretable the dataset. To optimally choose the number of PCs, several techniques have been created, such as the method of percentage of variance explained, using the Scree-plot or Cross-validation for PCA (described in [2. Cross-validation on PCA methods]).

The percentage of variance explained method is based on the dataset's covariance matrix, and the percentage of variance vs. the total variability (sum of diagonal entries of the original covariance matrix). When computing the PCs for a given dataset $\mathbf x_1,\ldots,\mathbf x_N \in \mathbb{R}^p$, we create an orthonormal basis with a corresponding diagonal covariance matrix with its entries being the eigenvalues of corresponding PCs. As we sequentially construct the components, the eigenvalues $\lambda_1, \lambda_2, ... ,\lambda_r \in \mathbb{R}$ will be a decreasing function. The method of percentage of variance explained is simply to select the first $r$ components such that $\sum_{i=1}^r \lambda_i > \tau V$ for an arbitrary treshold $\tau$ and a total variability $V = \sum_{i=1}^p \lambda_i$.

The Scree-plot is again related to the eigenvalues of the newly created orthonormal basis of PCs. As shown above, the eigenvalues corresponding to the PCs are a decreasing sequence, so we get a graph as below when plotting the PCs index vs the eigenvalue :
```{r, echo=F, out.width='50%', fig.align='center'}
knitr::include_graphics('./Figures/scree_plot_example.png')
```

The Scree-plot method consists of choosing the value of $r$ which is the elbow of the graph as seen above. In this example, the optimal value of $r$ is $3$.

### 1.3 Simulation dataset description

After the end of [2. Cross-validation on PCA methods] we will be equipped with multiple approaches that will enable us to recover the best amount of dimensions. Intuitively, every method must have its different advantages and draw backs.
In order to get a better feeling on how our Cross-Validation methods behave in different circumstances, we will run a simulation study on a variety of synthesized data sets and try to cover a good variety of covariance matrices. This will enable us to discover the limits of the different methods used and allow us to conclude and recommend different Algorithms with respect to its underlying data. 
As by construction of our Alogirthms and every single one of them besides the [2.1 Naïve Approach], the [2.5 Matrix completion method] they are only applicable to Multivariate Gaussian distributed data. There we will restrict ourselves to this special distribution and create our synthesized data in order to get meaningful results. All in all we will consider 8 different data sets and compare the outcome of our 5 Algorithms on every single one of them.

for finding $r$, the chosen simulation data set et will consist of a centered multivariate Gaussian data set $\mathbf x_1,\ldots,\mathbf x_N \in \mathbb{R}^p$ with a covariance matrix defined as below : ## ADD.


This choice of multivariate Gaussian is due to the constraints of implementation for methods described in [2. Cross-validation on PCA methods]. Without certain properties of Gaussian vectors, such as the expression conditional expectation from missed data given observed data (as seen in [2.2 Artificially turn the unsupervised problem into a supervised one]), certain methods would not be usable.

## 2. Cross-validation on PCA methods

```{r, echo=F, eval=F}
# Dataframe as input
df <- c()
# Create folds for Cross-validation
df_df <- data.frame("obs1"=c(5,43,2,3,1,4,8,4,5,9), "obs2"=c(5,6,7,3,9,9,12,1,5,8), "obs3"=c(1,2,5,3,4,5,6,1,5,7), "obs4"=c(1,7,6,3,4,5,6,1,7,10))
df <- as.matrix(df_df)
n <- dim(df)[1]
p <- dim(df)[2]
index <- c(1:n)
K <- 3
samples <- vector("list", K)
for (i in 1:K) {
  if (i < K) {
    fold <- sample(index, n/K)
    samples[[i]] <- fold
    #print(which(fold %in% index))
    index <- index[-which(index %in% fold)]
  }
  else{
    samples[[i]] <- index
  }
}
# Index can now be used to slice df in every step of Cross-validation

# How is r set? r < p
r <- 3
# Sample randomly r columns
trunc <- sample(1:p, p-r)
```

### 2.1 Naïve Approach

A widely used approach for Cross-validation in PCA is a method that has the right intentions but fails at a critical point. Imaging sliding down slide with no end. Gravity would make that an endless ride. Same concept is happening here. But due to computational issues the endlessness is adapted to finite. This is due to the fact that this Naïve Approach does not take into account any variance. Let the algorithm as presented in [Notes 06 - T. Masak](https://htmlpreview.github.io/?https://github.com/TMasak/StatComp/blob/master/Notes/06_CV.html#no.-of-component-selection-for-pca) speak for itself. We start with the usual conventions by denoting $X \in \mathbb{R}^{n \times p}$ our data with $n$ observations and $p$ variables:

  * split data into $K$ folds $J_1,\ldots,J_K$
  * **for** $k=1,\ldots,K$:
    - solve $\widehat{\mathbf{L}} = \underset{\mathrm{rank}(\mathbf L) = r}{\mathrm{arg\,min}} \|\mathbf X[J_k^c,] -\mathbf L \|_2^2$
    - calculate $Err_k(r) = \frac{1}{|J_k|}\sum_{m \in J_k} \| x^m - P_{\widehat{\mathbf{L}}} x^m \|_2^2$
  * **end for**
  * choose $\widehat{r} = \underset{r}{\mathrm{arg\,min}} \sum_{k=1}^K Err_k(r)$

A numerical method to solve the least square problem in the top of the algorithm would be to compute the SVD decomposition of $\mathbf X[J_k^c,]$ and truncate the $p-r$ smallest singular values. In order to estimate $\mathbf{x^{m}}, \forall m \in J_k$ with a lower dimensional version we project it by the matrix $P_{\mathbf{\widehat{L}}}$. Where does this matrix come from and how is it computable? To resolve this we took advantage of the R built-in function `prcomp()` the returns us among other objects a basis for the dimension of $\widehat{\mathbf L}$. The promised issue occurs when computing the our error measure without further to-do. Thus the error obviously shrinks when $\widehat{\mathbf L}$ is allowed to have higher dimensions and atteins $0$ when its number of dimensions equals those of the initial data set $\mathbf{X}$.

Indeed



### 2.2 Artificially turn the unsupervised problem into a supervised one

We now want to "correct" the [2.1 Naïve Approach] and artificially induce a Bias-Variance trade-off. As described in [Notes 06 - T. Masak](https://htmlpreview.github.io/?https://github.com/TMasak/StatComp/blob/master/Notes/06_CV.html#no.-of-component-selection-for-pca), we implement this method as below. Let again $\mathbf{X} \in \mathbb{R}^{n \times p}$ be our Gaussian data with $n$ observations and $p$ variables:

  * split data into $K$ folds $J_1,\ldots,J_K$
  * **for** $k=1,\ldots,K$:
    - Compute $\hat{\mu}$ and $\hat{\Sigma} := \hat{\Sigma}_r$ (the empirical estimator truncated to rank r) on $\mathbf{X}\setminus J_k$.
    - Split data points $\mathbf{x}^m\in \mathbb{R}^{p}, m \in J_k$ into a "missing" part $\mathbf{x}_{miss}$ that will be used for validation and an "observed" part $\mathbf{x}_{obs}$ in order to estimate its counterpart$
    - Predict the missing part from the observed part using the $\hat{\mu}$ and $\hat{\Sigma}_r$
    - That is compute $\hat{\mathbf{x}}^m_{miss} = \mathbb{E}_{\hat{\mu},\hat{\Sigma}_r}[\mathbf{X}^m_{miss}|\mathbf{X}^m_{obs}=\mathbf{x}^m_{obs}] = \hat{\mu}_{miss} + \hat{\Sigma}_{miss,obs}\hat{\Sigma}^+_{obs,obs}(\mathbf{x}^m_{obs}-\hat{\mu}_{obs})$, where $\hat{\Sigma}^+_{obs,obs}$ is considered the Pseudoinverse of $\hat{\Sigma}_{obs,obs}$ and $\hat{\mu}_{miss},\hat{\mu}_{obs}$ is $\hat{\mu}$ split according to the "missed" and "observed" part
  * **end for**
  * choose $\widehat{r} = \underset{r}{\mathrm{arg\,min}} \sum_{k=1}^K Err_k(r)$, where for instance $Err_k(r)$ can be considered as the MSE for every fold $J_k$, e.g. $Err_k(r)=\frac{1}{|J_k|}\sum_{m\in J_k} \|\hat{\mathbf{x}}^m_{miss}-\mathbf{x}^m_{miss}\|^2_2$

Before continuing with our simulations we want to emphasize on some more and some less obvious clues in order to avoid any confusion. When we truncate $\hat{\Sigma}$ to $\hat{\Sigma}_r$ in every fold, we actually take its SVD decomposition and "cut off" its $p-r$ smallest singular values. Hence, we reduce the rank of the covariance of $\mathbf{X}$, but we preserve its dimension which allows us to estimate $\mathbf{x}^m_{miss}$ in the later part of the algorithm. When splitting the observations in every iteration of the for loop, it naturally comes to mind if we should always take the same or a different (random) split. We went for the first possibility as that additional random factor would be negligible by the SLLN. Nevertheless, it makes sense to keep two equally sized parts in order to have a proper estimation basis without overestimating.
When we estimate $\hat{\mathbf{x}}^m_{miss}$ we use a specific formula that emerges from the law of our data set $\mathbf{X}$. Hence, it is only applicable in this form to Gaussian distributed observations $\mathbf{x} \in \mathbb{R}^p$. The estimation also makes use of the Pseudoinverse of $\hat{\Sigma}_{obs,obs}$ denoted by $\hat{\Sigma}^+_{obs,obs}$. We use this convention in order to avoid singularity conflicts. The Pseudoinverse is easily callable in R by the function `ginv()` embedded in the `MASS` package.

```{r, echo=F, eval=F}
# Cross-validation: "Artificially turn the unsupervised problem into a supervised one"

# Split data into missing and observed data on jth component
# e.g. split missing and observed data in half, or split variables randomly
# split <- sample(1:p, ceiling(ratio)), ratio < r as data is truncated to rank r

split <- 1:ceiling(r/2)

mse1 <- rep(0, K)

for (k in 1:K) {
  l1 <- length(samples[[k]])
  
  df_k <- df[-samples[[k]],]
  mu <- colMeans(df_k)
  mu_trunc <- mu[-trunc]
  sigma <- cov(df_k)
  sigma_trunc <- sigma[-trunc,-trunc]
  
  df_k_fold <- df[samples[[k]],]
  df_k_fold_trunc <- df_k_fold[,-trunc]
  df_k_fold_miss <- as.matrix(df_k_fold_trunc[,split])
  df_k_fold_obs <- as.matrix(df_k_fold_trunc[,-split])
  
  # Estimate est_x_miss for df_k_fold_miss
  mu_miss <- mu_trunc[split]
  mu_obs <- mu_trunc[-split]
  sigma_miss_obs <- sigma_trunc[split, -split]
  sigma_obs_obs <- sigma_trunc[-split, -split]

  est_x_miss <- lapply(1:l1, function(n){mu_miss+sigma_miss_obs%*%solve(sigma_obs_obs)%*%(df_k_fold_obs[n,]-mu_obs)})
  
  # Error of estimated and true missing observation
  for(s in 1:l1){
    mse1[k] <- mse1[k] + norm(est_x_miss[[s]]-df_k_fold_miss[s,], type = "2")^2/l1/K
  }
}
```
### 2.3 Missing data approach

```{r echo=F, eval=F}

# Cross-validation: "Missing Data Approach"

# Split data into missing and observed data on jth component
# e.g. split missing and observed data in half, or split variables randomly
# splitting the data can be done for all variables regardless of r
# split <- sample(1:p, ceiling(ratio)), ratio !< r
split <- 1:ceiling(r/2)

mse2 <- rep(0, K)

for (k in 1:K) {
  l1 <- length(samples[[k]])
  
  df_k <- df[-samples[[k]],]
  mu <- colMeans(df_k)
  svd_sigma <- svd(cov(df_k))
  sigma_trunc <- svd_sigma$u[,-trunc] %*% diag(svd_sigma$d[-trunc]) %*% t(svd_sigma$u[,-trunc])
  cov(df_k)
  
  df_k_fold <- df[samples[[k]],]
  df_k_fold_miss <- as.matrix(df_k_fold[,split])
  df_k_fold_obs <- as.matrix(df_k_fold[,-split])
  
  # Estimate est_x_miss for df_k_fold_miss
  mu_miss <- mu[split]
  mu_obs <- mu[-split]
  sigma_miss_obs <- sigma_trunc[split, -split]
  sigma_obs_obs <- sigma_trunc[-split, -split]
  
  est_x_miss <- lapply(1:l1, function(n){mu_miss+sigma_miss_obs%*%solve(sigma_obs_obs)%*%(df_k_fold_obs[n,]-mu_obs)})
  
  # Error of estimated and true missing observation
  for(s in 1:l1){
    mse2[k] <- mse2[k] + norm(est_x_miss[[s]]-df_k_fold_miss[s,], type = "2")^2/l1/K
  }
  
}

mse1
mse2
```


### 2.4 KDE modified approach


### 2.5 Matrix completion method

This final methods is somehow a bit more general, as we do not need to assume any probability distribution of the data $X \in \mathbb{R}^{N\times p}$, but just that $X$ has a finite rank. Indeed, for a given data matrix $X$, we assume that a portion of this matrix is missed and that the remaining elements are observed. Let $\Omega$ be a bivariate index set for the observed data in $X$. The matrix completion method consists of finding a matrix $\mathbf M$ of rank $R$ such that :

\[
    \mathrm{arg \, min}_{\mathbf{M}=(M_{ij})_{i,j=1}^{N \times p}} \sum_{(i,j) \in \Omega} (X_{ij} - M_{ij})^2 \quad \mathrm{s.t.} \quad \mathrm{rank}(\mathbf M)=R
    \]

This optimization method gives us the best matrix $\mathbf M$ with rank $R$ with respect to least-square error on the observed data. However, finding the solution of such a optimization problem is $NP$-hard, so one can use the following iterative algorithm to find local best candidates for the matrix $\mathbb M$ :


  * Select an initial candidate $\mathbf{M}^{(0)}$ (here $\mathbf{M}^{(0)} = X$)
  * Set $l=1$
  * **do**
    - Compute the SVD of $\mathbf{M}^{(l-1)}$, i.e. $\mathbf{M}^{(l-1)} = UDV^T$
    - Copy $D$ into $\widetilde{D}$ and set the $p-R$ diagonal elements of $\widetilde{D}$ to zero (truncating to rank $R$)
    - Set $\widetilde{M} = U\widetilde{D}V^T$, the new matrix of rank $R$.
    - Set $\mathbf{M}^{(l)}$as : 
    \[
      \mathbf{M}^{(l)} = \begin{cases} X_{ij} \quad \text{for } (i,j) \in \Omega \\
                                       \widetilde{M}_{i,j} \quad \text{for } (i,j) \notin \Omega \end{cases}
      \]
  * **while $||M^{(l)}-M^{(l-1)}||_2> tol$**:
  * Store the error $\sum_{(i,j) \in \Omega} (X_{ij} - M^{(l)}_{ij})^2$ and then repeat this experiment with different values of $R$. Pick the $R$ which gives the lowest error.
    
## 3. Comparison of CV methods

## 4. Other dimension-reduction techniques and comparison with CV

### 4.1 Percentage of variance explained method

### 4.2 Scree-plot method (non-automatic)

### 4.3 Comparison with CV methods

## 5. Conclusion