---
title: "Stash"
author: "Noah Scheider"
date: "2022-12-23"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Fixing our data set to be $D_0$ and applying our CV Methods, we get very different results for each algorithm. Please note that the $y$-axis in in logscale in order to get meaningful insights about the errors, especially for the [Missing Data approach]. First, the error of the [Naïve Approach] decreases continuously and achieves its best estimation when allowing the method to make use of all $8$ variables, as theoretically expected.

The Wrong PCA Method Improved method will however counter this behavior and the error will rise after hitting the optimal $r=3$ value. Overall this would lead the Wrong PCA Improved Method to favor rank $3$ as estimate for the rank of our modified data set. In general we can still see that the Wrong PCA Improved has difficulties to extract the exact amount of variables, especially when the added noise to the data set is constantly high, as the high curve in yellow is less steep than the blue one with low noise. We also note that for the high noise curve, there is a tendency to revert back when $r$ increases, which might cause problems as the biggest value $r$ might be returned.

The [Missing data approach] differs in its overall behavior from the previous to methods. While the error only decreases slightly in the first and the second variable, it drops noticeably for $r=3$ in our Simulation Study especially for low and differing noise. Exceeding rank $3$, there is a strong increase in the error due to the $\Sigma$ estimation of the EM-Algorithm diverging from the expected value. A possible reason for this divergence is the choice of initial covariance matrix (in our case the Identity matrix). Here, we conclude the this approach also finds the correct dimension, but one has to be careful of the amount of noise added as this challenges the algorithm quite a bit.

The error measure of the [Matrix completion method] seems to be a mix of the error of the Wrong PCA Improved and the Missing Data approach. We observe a slight increase in the first two variables, followed by a strong error drop on variable $3$ before it starts to increase again. The increase after variable $3$ is clearly stronger than at the beginning.

As for the [KDE Modified Approach], it will be analysed at the end of this section due to similar behaviors with respect to different datasets.

After this initial analysis, one would like to examine how the CV methods generalize to other datasets, i.e. moving from the identity matrix as covariance to a random matrix. Hereafter are the results for $D_1$, being the dataset with a random covariance matrix, as defined in the [Simulation dataset description]

```{r Noise1, echo=FALSE,out.width="80%", fig.align = 'center', fig.width=12,fig.height=8,fig.cap = "Comparison of CV methods on base 1 dataset with different noises as indicated by Noise°1,2 and 3"}
par(mfrow=c(2,2))
plot_dataset_together("Wrong PCA",1,T)
plot_dataset_together("Wrong PCA Improved",1,T)
plot_dataset_together("Missing Data",1,T)
plot_dataset_together("Matrix Completion",1,T)
```

<<<<<<< HEAD
The [Naive Approach] seems to preserve the same shape and error size as on $D_0$. The Wrong PCA Improved method slightly changes as it still drops in within the first three variables, but stabilizes more strongly afterwards. Again we see how the total noise added accounts for the different stabilization errors. For the Missing Data Approach error we observe a strong similarity to the previous analysis. Its error starts to decrease until variable $3$ and then spikes up to stabilize. The only noticable difference is, that the total amount of noise added plays now a stronger criterion on where the curve converges. With the uniform low noise converging with the lowest error. In the Matrix Completion method we now observe that the algorithms doesn't screw up its prediction with high uniform noise, but does this time for differing noise. For high noise the method favors $3$ variables, where as it chooses $2$ variables for differing noise. For a uniform low noise it seems to work well by, having a sharp decrease in error rank $3$ and then increasing again.

We now examine to which extent our algorithms change if we consider a structured covariance $\Sigma_2$, with increasing impact on the last variables.
=======
The [Naïve Approach] seems to preserve the same shape and error size as on $D_0$. The Wrong PCA Improved method slightly changes as it still drops in within the first three variables, but stabilizes more strongly afterwards especially for Noise n°3. Again we see how the total noise added accounts for the different stabilization error. Interestingly, the Missing Data Approach  and the Matrix Completion Method aren't able to return more precise estimation with a higher amount of variables and actually increase in error size. Both have kind of the same shape and error size, whereas the latter one seems to be a smoother version of the first one. Also the error curve sticks closely together for the uniform noise in the Missing Data Approach, whereas the differing noise starts beneath the  and starts beneath 
>>>>>>> 42feed500abe249bc11b9e390e35b03af0f321e8

```{r Noise2, echo=FALSE,out.width="80%", fig.align = 'center', fig.width=12,fig.height=8,fig.cap = "Comparison of CV methods on base 2 dataset with different noises as indicated by Noise°1,2 and 3"}
par(mfrow=c(2,2))
plot_dataset_together("Wrong PCA",2,T)
plot_dataset_together("Wrong PCA Improved",2,T)
plot_dataset_together("Missing Data",2,T)
plot_dataset_together("Matrix Completion",2,T)
```

<<<<<<< HEAD
The Wrong PCA has behaves again the same regardless of the noise added. The first observation also applies for the Wrong PCA improved. The only subtlety we want to emphasize on, is that it seems once again to be sensitive with the total amount of noise added. A stronger decrease in the first two variables for the uniform and differing noise is observable.
Interestingly, the Missing Data Approach  and the Matrix Completion Method don't seem to handle the kind of structure very well. In fact the algorithms aren't able to return more precise estimation with a higher amount of variables and actually increase in error size. Both have kind of the same shape and error size, whereas the latter one seems to be a smoother version of the first one. Also the error curve sticks closely together for the uniform noise in the Missing Data Approach, whereas the differing noise starts beneath the other two error curves but then exceed them after rank $5$. The adhesivity is somewhat stronger for the error  Matrix completion error as the error curve is almost identical regardless of the noise added.

```{r NoiseMissing0, echo=FALSE,out.width="80%", fig.align = 'center', fig.width=12,fig.height=4,fig.cap = "KDE Approach for base 0 dataset with different noises as indicated by Noise°1,2 and 3"}
plot_three("KDE Approach",0)
=======
```{r NoiseKDE, echo=FALSE,out.width="80%", fig.align = 'center', fig.width=12,fig.height=4,fig.cap = "KDE Modified Approach for the 3 datasets with different noises as indicated by Noise°1,2 and 3"}
par(mfrow=c(1,3))
plot_dataset_together("KDE Approach",0,F)
plot_dataset_together("KDE Approach",1,F)
plot_dataset_together("KDE Approach",2,F)
>>>>>>> 42feed500abe249bc11b9e390e35b03af0f321e8
```


For the [KDE Modified Approach], we observe in general that for every variable until the third one it drops very strongly. Having reached rank $3$ the quality of its estimation seem to stay the same and correspondingly the error. In case of the second data set we observe the same behaviour but instead of stabilizing after rank $3$ it does that after reaching variable $2$ by construction of our data set.





## Comparison across different covariance matrices 

We now fix the error to be the third one and analyze how our different CV methods behave on the different kinds of base data sets, e.g. $D_0^3, D_1^3$ and $D_2^3$. We only consider the third kind of noise as we consider it to be a mix between the uniform high and low noise $\epsilon^1, \epsilon^2$ and the plots only differ negligibly but stay consistent in their fundamental shape for the non-mentioned noise. We also want to mention that the following plots are the same plots as before, only rearranged in order to have a better representation on how our CV Methods behave with respect to the different base data sets.

Unsurprisingly, the shape of the [Naïve Approach] doesn't change at all. However, we are able to observe that the error level is slightly different until variable $3$ but then really sticks together.

The [Artificially turn the unsupervised problem into a supervised one] again decreases until the true value of the data set and then stabilized. The error is the lowest for the data set with a structured data set, then for the one with an Identity Matrix as covariance and the worst being the one with a random covariance matrix.

The [Missing data approach] gives us a similar shape for $D_0$ and $D_1$ but a fundamentally different one for $D_2$. Whereas the error again decreases until rank $3$ for $D_0$ and $D_1$, it doesn't seem to be the case for $D_2$. This approach doesn't find the true rank for $D_2$ and keeps increasing continuously from the beginning until the last three variables. For $D_0$ and $D_1$ we notice the same sharp increase in error size when exceeding the true rank of the data set.

For the [Matrix completion method] we get similar characteristics for $D_0$ and $D_1$, whereas the method fails at $D_2$. Evaluating it on $D_0$ and $D_1$ we observe a slight increase in the fist two variables but then a noticeable error decrease when allowing the method to truncate to the true rank of data data set. Looking carefully at the plot we see that the method actually overshoots the true rank for $D_1$ as it get the best estimations at the fourth variable. Exceeding the third and fourth variable respectively, the error reincreases but stabilizes beneath the initial error size of variable $1$ and $2$. For $D_2$ we monitor a convex, square root like function favoring dimension $1$ to be the true rank of the data set even though we would have expected it to be $2$. $D_1$ and $D_2$ seem to be a source of trouble as they don't allow us to conclude the true rank based on the error development within the variables.